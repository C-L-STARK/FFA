import{_ as e,c as o,o as a,a3 as t,j as n}from"./chunks/framework.CVqs_p0L.js";const b=JSON.parse('{"title":"Free Zhipu Qingyan Large Model glm-4v-9b","description":"","frontmatter":{"layout":"doc"},"headers":[],"relativePath":"en/chat/glm-4v-9b.md","filePath":"en/chat/glm-4v-9b.md","lastUpdated":1719800650000}'),i={name:"en/chat/glm-4v-9b.md"},r=t('<h1 id="free-zhipu-qingyan-large-model-glm-4v-9b" tabindex="-1">Free Zhipu Qingyan Large Model glm-4v-9b <a class="header-anchor" href="#free-zhipu-qingyan-large-model-glm-4v-9b" aria-label="Permalink to &quot;Free Zhipu Qingyan Large Model glm-4v-9b&quot;">​</a></h1><p>Zhipu Large Model, full name: Zhipu Qingyan, <a href="https://ChatGLM.cn/">official website</a></p><p>Currently, Zhipu Qingyan offers <code>25 million free Tokens</code> upon registration. <code>If you are a user of the OpenAI API, by providing a screenshot to their customer service email, you can also receive a special offer of 5 yuan for 100 million tokens.</code></p><p>Moreover, the Zhipu Qingyan large model now supports internet connectivity and multimodality. Although its capabilities are not as strong as gpt-4o, it is much better than gpt-3.5-turbo.</p><p>The 25 million free tokens are valid for a default of 1 month; enough for everyone to enjoy for a month. If you are lucky enough to purchase 100 million tokens (valid for 3 months), you can even share them generously with friends!</p><p>The method of use is very simple. Find the open-source WebUI on github, fill in the key, and you can enjoy it immediately.</p><p>Here we especially recommend lobe-chat. To avoid key leaks, please use the official deployment. Conveyor belt: <a href="https://chat-preview.lobehub.com/" target="_blank">lobe-chat</a></p><h2 id="configuration-process" tabindex="-1">Configuration Process <a class="header-anchor" href="#configuration-process" aria-label="Permalink to &quot;Configuration Process&quot;">​</a></h2><ol><li><p>Use the conveyor belt to open or search on <code>github</code> for local deployment and startup.</p></li><li><p>Enter the chat interface, click on the top right corner to open the configuration interface, and then find the language model configuration item.</p></li></ol><p><img src="https://oss.fastx-ai.com/file/upload/2024/06/30/1807404146362421248.png" alt=""></p><ol start="3"><li>Return to the chat interface, select the appropriate model, and start chatting!</li></ol><h2 id="free-online-experience-loading-may-be-slow-requires-magic" tabindex="-1">Free Online Experience (Loading may be slow, requires magic) <a class="header-anchor" href="#free-online-experience-loading-may-be-slow-requires-magic" aria-label="Permalink to &quot;Free Online Experience (Loading may be slow, requires magic)&quot;">​</a></h2><p>Of course, you can also use the online experience interface for online experience. The <code>loading may be slow, please be patient</code>, and it&#39;s best to have an account on <a href="https://huggingface.co/" target="_blank" rel="noreferrer">huggingface</a>.</p>',13),l=n("iframe",{src:"https://vilarin-vl-chatbox.hf.space",frameborder:"0",width:"100%",height:"1000"},null,-1),c=[r,l];function s(h,p,d,u,g,f){return a(),o("div",null,c)}const y=e(i,[["render",s]]);export{b as __pageData,y as default};
